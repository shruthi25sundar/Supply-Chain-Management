{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9b16e8-270c-4400-a05b-8ff32a69a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b827ed-d3b7-47fc-ad29-b48a15dcba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"/workspaces/Supply-Chain-Management/Data/supplier_contracts_dataset.csv\")\n",
    "df = df.replace({np.nan: None})\n",
    "# Rename columns to remove spaces\n",
    "# df.columns = [col.replace(' ', '_') for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb664984-a7ef-4200-b6b5-3e4038f6141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns = df.columns.str.lower()\n",
    "# df.to_csv(r\"/workspaces/Supply-Chain-Management/Data/supplier_contracts_dataset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba4aa868-e50f-42a6-83c1-edad05481382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['supplier_id', 'supplier_name', 'supplier_type', 'risk_level',\n",
       "       'compliance_issues', 'key_terms', 'past_performance',\n",
       "       'negotiate_recommendation', 'supply_chain_disruption',\n",
       "       'quality_metrics', 'cost_metrics'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0882f38e-bfb0-4a6d-8e86-b800e487995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.rename(columns={'supplier_id':'id'})\n",
    "# # Convert only the relevant text-based fields to string\n",
    "text_fields = ['supplier_name', 'supplier_type', 'risk_level',\n",
    "       'compliance_issues', 'key_terms', 'past_performance',\n",
    "       'negotiate_recommendation', 'supply_chain_disruption',\n",
    "       'quality_metrics', 'cost_metrics']\n",
    "\n",
    "# Ensure the specified text fields are strings\n",
    "for field in text_fields:\n",
    "    df[field] = df[field].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f92f7-1a8a-4269-8b50-73344ed859c7",
   "metadata": {},
   "source": [
    "**MINSEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6183ad8-eecb-4b0c-89d5-88e50fcd868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns = df.columns.str.lower()\n",
    "# df.to_csv(r\"/workspaces/Supply-Chain-Management/Data/supplier_contracts_dataset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a10368-dd4e-4f83-b5c1-0188e0a964d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300178f7-1cb6-46de-8d96-5375400164ea",
   "metadata": {},
   "source": [
    "Get top 10 list of high risk level contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b9cd5b-c404-476c-9390-2bf2f2dc600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'S2482', 'supplier_name': 'Supplier 2482', 'supplier_type': 'Service Provider', 'risk_level': 'High', 'compliance_issues': 'Non-Compliance with Standards', 'key_terms': '45-day payment, 10-day delivery', 'past_performance': 'Poor', 'negotiate_recommendation': 'Include penalty clauses for late delivery, Include compliance monitoring, Adjust delivery schedules', 'supply_chain_disruption': 'No', 'quality_metrics': '2.99% defect rate, Meets standards', 'cost_metrics': '$60.18/unit, $5539.99 total cost'}\n",
      "{'id': 'S2481', 'supplier_name': 'Supplier 2481', 'supplier_type': 'Manufacturer', 'risk_level': 'High', 'compliance_issues': 'Substandard Quality', 'key_terms': '30-day payment, 5-day delivery', 'past_performance': 'Good', 'negotiate_recommendation': 'Include penalty clauses for late delivery, Seek alternative suppliers, Adjust delivery schedules', 'supply_chain_disruption': 'No', 'quality_metrics': '1.04% defect rate, Meets standards', 'cost_metrics': '$66.06/unit, $5158.25 total cost'}\n",
      "{'id': 'S2480', 'supplier_name': 'Supplier 2480', 'supplier_type': 'Distributor', 'risk_level': 'High', 'compliance_issues': 'Late Delivery', 'key_terms': '30-day payment, 5-day delivery', 'past_performance': 'Poor', 'negotiate_recommendation': 'Adjust delivery schedules, Include compliance monitoring, Include penalty clauses for late delivery', 'supply_chain_disruption': 'Yes', 'quality_metrics': '3.96% defect rate, Meets standards', 'cost_metrics': '$67.89/unit, $5083.32 total cost'}\n",
      "{'id': 'S2476', 'supplier_name': 'Supplier 2476', 'supplier_type': 'Service Provider', 'risk_level': 'High', 'compliance_issues': 'Late Delivery', 'key_terms': '60-day payment, 7-day delivery', 'past_performance': 'Fair', 'negotiate_recommendation': 'Include compliance monitoring, Include penalty clauses for late delivery, Seek alternative suppliers', 'supply_chain_disruption': 'No', 'quality_metrics': '3.92% defect rate, Meets standards', 'cost_metrics': '$58.74/unit, $5201.92 total cost'}\n",
      "{'id': 'S2475', 'supplier_name': 'Supplier 2475', 'supplier_type': 'Service Provider', 'risk_level': 'High', 'compliance_issues': 'None', 'key_terms': '30-day payment, 5-day delivery', 'past_performance': 'Poor', 'negotiate_recommendation': 'Adjust delivery schedules, Include compliance monitoring, Include penalty clauses for late delivery', 'supply_chain_disruption': 'Yes', 'quality_metrics': '2.36% defect rate, Meets standards', 'cost_metrics': '$56.24/unit, $5756.96 total cost'}\n",
      "{'id': 'S2471', 'supplier_name': 'Supplier 2471', 'supplier_type': 'Distributor', 'risk_level': 'High', 'compliance_issues': 'Substandard Quality', 'key_terms': '45-day payment, 10-day delivery', 'past_performance': 'Poor', 'negotiate_recommendation': 'Include penalty clauses for late delivery, Include compliance monitoring, Seek alternative suppliers', 'supply_chain_disruption': 'Yes', 'quality_metrics': '4.0% defect rate, Meets standards', 'cost_metrics': '$63.12/unit, $5305.8 total cost'}\n",
      "{'id': 'S2470', 'supplier_name': 'Supplier 2470', 'supplier_type': 'Retailer', 'risk_level': 'High', 'compliance_issues': 'None', 'key_terms': '60-day payment, 7-day delivery', 'past_performance': 'Fair', 'negotiate_recommendation': 'Include compliance monitoring, Include penalty clauses for late delivery, Seek alternative suppliers', 'supply_chain_disruption': 'No', 'quality_metrics': '3.64% defect rate, Meets standards', 'cost_metrics': '$63.19/unit, $5607.82 total cost'}\n",
      "{'id': 'S0032', 'supplier_name': 'Supplier 32', 'supplier_type': 'Retailer', 'risk_level': 'High', 'compliance_issues': 'Non-Compliance with Standards', 'key_terms': '60-day payment, 7-day delivery', 'past_performance': 'Excellent', 'negotiate_recommendation': 'Include penalty clauses for late delivery, Seek alternative suppliers, Adjust delivery schedules', 'supply_chain_disruption': 'No', 'quality_metrics': '1.48% defect rate, Meets standards', 'cost_metrics': '$56.9/unit, $5324.41 total cost'}\n",
      "{'id': 'S0028', 'supplier_name': 'Supplier 28', 'supplier_type': 'Retailer', 'risk_level': 'High', 'compliance_issues': 'Late Delivery', 'key_terms': '30-day payment, 5-day delivery', 'past_performance': 'Fair', 'negotiate_recommendation': 'Seek alternative suppliers, Include compliance monitoring, Include penalty clauses for late delivery', 'supply_chain_disruption': 'No', 'quality_metrics': '3.01% defect rate, Meets standards', 'cost_metrics': '$62.42/unit, $5753.75 total cost'}\n",
      "{'id': 'S0026', 'supplier_name': 'Supplier 26', 'supplier_type': 'Distributor', 'risk_level': 'High', 'compliance_issues': 'Late Delivery', 'key_terms': '30-day payment, 5-day delivery', 'past_performance': 'Excellent', 'negotiate_recommendation': 'Include compliance monitoring, Seek alternative suppliers, Adjust delivery schedules', 'supply_chain_disruption': 'Yes', 'quality_metrics': '4.27% defect rate, Meets standards', 'cost_metrics': '$65.79/unit, $5685.45 total cost'}\n"
     ]
    }
   ],
   "source": [
    "import minsearch\n",
    "\n",
    "# Create an index\n",
    "index = minsearch.Index(\n",
    "    text_fields=text_fields,\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "# Fit the index with the documents\n",
    "index.fit(documents)\n",
    "\n",
    "# Example search query\n",
    "query = \"high risk level\"\n",
    "results = index.search(query, num_results=10)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c74c41-7f4e-4e5b-b043-cb561d3545ff",
   "metadata": {},
   "source": [
    "Get the Contract types that has high risk and their count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "884b4c83-5084-4261-8294-d96ccc7d5f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of each supplier type with high risk level:\n",
      "Retailer: 197\n",
      "Distributor: 198\n",
      "Manufacturer: 201\n",
      "Service Provider: 219\n"
     ]
    }
   ],
   "source": [
    "import minsearch\n",
    "from collections import Counter\n",
    "# Create an index\n",
    "index = minsearch.Index(\n",
    "    text_fields=text_fields,\n",
    "    keyword_fields=['risk_level']\n",
    ")\n",
    "\n",
    "# Fit the index with the documents\n",
    "index.fit(documents)\n",
    "\n",
    "# Perform the search for high-risk level contracts\n",
    "filter_dict = {'risk_level': 'High'}\n",
    "results = index.search(query='high risk level', filter_dict=filter_dict, num_results=len(documents))\n",
    "\n",
    "# Extract and print contract types with high risk level\n",
    "high_risk_contract_types = [result['supplier_type'] for result in results]\n",
    "\n",
    "# Count the occurrences of each contract type\n",
    "contract_type_counts = Counter(high_risk_contract_types)\n",
    "\n",
    "# Print the count of each contract type\n",
    "print(\"Count of each supplier type with high risk level:\")\n",
    "for contract_type, count in contract_type_counts.items():\n",
    "    print(f\"{contract_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba7a3d-375a-446f-99f1-784b93b04861",
   "metadata": {},
   "source": [
    "**GROQ API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4215677c-21b6-421a-a293-51059466e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dcc07c-a334-4093-a931-a8d36579dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supplier Types: Manufacturer, Retailer, Distributor, Service Provider\n",
      "Quality Metrics: Defect Rate, Meets Standards\n",
      "Supply Chain Disruptions: Yes, No\n",
      "Negotiation Recommendations: Seek alternative suppliers, Include penalty clauses for late delivery, Adjust delivery schedules, Include compliance monitoring\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to perform the search on supplier contracts based on query\n",
    "\n",
    "def search(query, filter_dict=None, max_results=10):\n",
    "    # Filter the DataFrame based on risk level (if provided)\n",
    "    if filter_dict:\n",
    "        filtered_df = df[df['risk_level'] == filter_dict.get('risk_level', '')]\n",
    "    else:\n",
    "        filtered_df = df\n",
    "    # Convert the filtered data to a list of dictionaries and limit the number of results\n",
    "    results = filtered_df.to_dict(orient='records')[:max_results]\n",
    "    return results\n",
    "\n",
    "# Function to build a clearer prompt for Groq API\n",
    "def build_clear_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context += (\n",
    "            f\"- **Supplier_Type**: {doc['supplier_type']}\\n\"\n",
    "            f\"  **Supplier_Name**: {doc['supplier_name']}\\n\"\n",
    "            f\"  **Risk_Level**: {doc['risk_level']}\\n\"\n",
    "            f\"  **Compliance_Issues**: {doc['compliance_issues']}\\n\"\n",
    "            f\"  **Key_Terms**: {doc['key_terms']}\\n\"\n",
    "            f\"  **Negotiate_Recommendation**: {doc['negotiate_recommendation']}\\n\"\n",
    "            f\"  **Quality_Metrics**: {doc['quality_metrics']}\\n\"\n",
    "            f\"  **Past_Performance**: {doc['past_performance']}\\n\"\n",
    "            f\"  **Supply_Chain_Disruption**: {doc['supply_chain_disruption']}\\n\"\n",
    "            f\"  **Cost_Metrics**: {doc['cost_metrics']}\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    prompt = (\n",
    "        f\"QUESTION: {query}\\n\\n\"\n",
    "        f\"CONTEXT:\\n{context}\"\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Function to call the LLM (Groq API)\n",
    "def llm(prompt, model='Llama3-groq-70b-8192-tool-use-preview'):\n",
    "    # Assuming client is the Groq API client instance\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Function to perform the full RAG (Retrieve and Generate) process\n",
    "def rag(query, model='Llama3-groq-70b-8192-tool-use-preview'):\n",
    "    # Search for high-risk contracts (you can modify filter_dict based on needs)\n",
    "    search_results = search(query, filter_dict={'risk_level': 'High'})\n",
    "    \n",
    "    # Build the prompt using the search results\n",
    "    prompt = build_clear_prompt(query, search_results)\n",
    "    \n",
    "    # Get the LLM response based on the prompt\n",
    "    answer = llm(prompt, model=model)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "question = \"Give supplier types, quality metrics, supply chain disruptions, and their negotiation recommendations for high-risk contracts\"\n",
    "answer = rag(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da304f86-6d92-490a-8769-2aa75162d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Risk-Based Queries\n",
      "Based on the provided data, the suppliers with the most non-compliance issues, regardless of risk level, are:\n",
      "\n",
      "1. Supplier 6 (Manufacturer) - Non-Compliance with Standards\n",
      "2. Supplier 8 (Retailer) - Non-Compliance with Standards\n",
      "3. Supplier 12 (Distributor) - Substandard Quality\n",
      "4. Supplier 14 (Service Provider) - Non-Compliance with Standards\n",
      "5. Supplier 15 (Distributor) - Non-Compliance with Standards\n",
      "6. Supplier 26 (Distributor) - Late Delivery\n",
      "7. Supplier 28 (Retailer) - Late Delivery\n",
      "8. Supplier 32 (Retailer) - Non-Compliance with Standards\n",
      "9. Supplier 38 (Retailer) - Late Delivery\n",
      "10. Supplier 40 (Retailer) - Non-Compliance with Standards\n",
      "\n",
      "## Compliance & Legal Queries:\n",
      "Based on the provided data, the suppliers where compliance monitoring is recommended are:\n",
      "\n",
      "1. Supplier 12 (Distributor) - High risk level, compliance issues with substandard quality.\n",
      "2. Supplier 40 (Retailer) - High risk level, compliance issues with non-compliance with standards.\n",
      "\n",
      "## Cost and Financial Metrics:\n",
      "Supplier 6, Supplier 8, Supplier 12, Supplier 14, Supplier 15, Supplier 26, Supplier 28, Supplier 32, Supplier 38, Supplier 40\n",
      "\n",
      "## Contractual Terms and Recommendations:\n",
      "Based on the provided context, the suppliers with penalty clauses for late delivery in their contracts are:\n",
      "\n",
      "1. Supplier 6\n",
      "2. Supplier 8\n",
      "3. Supplier 12\n",
      "4. Supplier 14\n",
      "5. Supplier 15\n",
      "6. Supplier 26\n",
      "7. Supplier 28\n",
      "8. Supplier 32\n",
      "9. Supplier 38\n",
      "10. Supplier 40\n",
      "\n",
      "The associated risks for these suppliers include:\n",
      "\n",
      "1. Supplier 6: High risk due to non-compliance with standards and supply chain disruption.\n",
      "2. Supplier 8: High risk due to non-compliance with standards and supply chain disruption.\n",
      "3. Supplier 12: High risk due to substandard quality and supply chain disruption.\n",
      "4. Supplier 14: High risk due to non-compliance with standards and supply chain disruption.\n",
      "5. Supplier 15: High risk due to non-compliance with standards and no supply chain disruption.\n",
      "6. Supplier 26: High risk due to late delivery and supply chain disruption.\n",
      "7. Supplier 28: High risk due to late delivery and no supply chain disruption.\n",
      "8. Supplier 32: High risk due to non-compliance with standards and no supply chain disruption.\n",
      "9. Supplier 38: High risk due to late delivery and supply chain disruption.\n",
      "10. Supplier 40: High risk due to non-compliance with standards and no supply chain disruption.\n",
      "\n",
      "## Supplier Relationship Queries:\n",
      "The relationship metrics for suppliers with the best past performance scores are as follows:\n",
      "\n",
      "1. **Supplier 14** (Service Provider): Excellent past performance, 2.45% defect rate, $69.82/unit, $6924.27 total cost.\n",
      "2. **Supplier 26** (Distributor): Excellent past performance, 4.27% defect rate, $65.79/unit, $5685.45 total cost.\n",
      "3. **Supplier 32** (Retailer): Excellent past performance, 1.48% defect rate, $56.9/unit, $5324.41 total cost.\n",
      "4. **Supplier 38** (Retailer): Excellent past performance, 4.68% defect rate, $57.93/unit, $6368.93 total cost.\n",
      "5. **Supplier 40** (Retailer): Excellent past performance, 1.76% defect rate, $66.89/unit, $6361.98 total cost.\n",
      "\n",
      "## Opportunity and Innovation Queries:\n",
      "Based on the provided context, the suppliers with innovative solutions despite having poor quality metrics are:\n",
      "\n",
      "1. Supplier 8: This supplier has a high risk level, non-compliance with standards, and a 4.38% defect rate. However, they have a good past performance and are experiencing supply chain disruptions.\n",
      "\n",
      "2. Supplier 12: This supplier also has a high risk level, compliance issues related to substandard quality, and a 3.84% defect rate. They have a poor past performance but are experiencing supply chain disruptions.\n",
      "\n",
      "3. Supplier 26: With a high risk level, compliance issues related to late delivery, and a 4.27% defect rate, this supplier has an excellent past performance and is experiencing supply chain disruptions.\n",
      "\n",
      "4. Supplier 38: This supplier has a high risk level, compliance issues related to late delivery, and a 4.68% defect rate. They have an excellent past performance and are experiencing supply chain disruptions.\n",
      "\n",
      "5. Supplier 40: This supplier has a high risk level, non-compliance with standards, and a 1.76% defect rate. They have an excellent past performance and are not experiencing any supply chain disruptions.\n",
      "\n",
      "# Custom Queries\n",
      "Based on the given context, the suppliers with a combination of poor quality metrics, high compliance issues, and good past performance scores are:\n",
      "\n",
      "1. Supplier 6\n",
      "2. Supplier 8\n",
      "3. Supplier 12\n",
      "4. Supplier 14\n",
      "5. Supplier 26\n",
      "6. Supplier 28\n",
      "7. Supplier 32\n",
      "8. Supplier 38\n",
      "9. Supplier 40\n",
      "\n",
      "\n",
      "The patterns between compliance issues and cost metrics in supplier contracts can be observed in the following ways:\n",
      "\n",
      "1. **Non-Compliance with Standards**: High-risk suppliers with compliance issues related to standards tend to have higher cost metrics. For example, Supplier 6, Supplier 8, Supplier 32, and Supplier 40 have higher cost metrics ($61.39/unit, $64.15/unit, $56.9/unit, and $66.89/unit, respectively) compared to other suppliers.\n",
      "\n",
      "2. **Late Delivery**: Suppliers with compliance issues related to late delivery tend to have higher cost metrics. For example, Supplier 26, Supplier 28, and Supplier 38 have higher cost metrics ($65.79/unit, $62.42/unit, and $57.93/unit, respectively) compared to other suppliers.\n",
      "\n",
      "3. **Defect Rate**: Suppliers with higher defect rates tend to have higher cost metrics. For example, Supplier 8, Supplier 26, and Supplier 38 have higher cost metrics ($64.15/unit, $65.79/unit, and $57.93/unit, respectively) and higher defect rates (4.38%, 4.27%, and 4.68%, respectively) compared to other suppliers.\n",
      "\n",
      "4. **Supply Chain Disruption**: Suppliers experiencing supply chain disruptions tend to have higher cost metrics. For example, Supplier 6, Supplier 12, Supplier 14, and Supplier 38 have higher cost metrics ($61.39/unit, $52.47/unit, $69.82/unit, and $57.93/unit, respectively) and have experienced supply chain disruptions.\n",
      "\n",
      "5. **Risk Level**: High-risk suppliers tend to have higher cost metrics. For example, all high-risk suppliers have higher cost metrics compared to low-risk suppliers.\n",
      "\n",
      "6. **Negotiation Recommendations**: Suppliers with recommendations to seek alternative suppliers, include penalty clauses for late delivery, and adjust delivery schedules tend to have higher cost metrics. For example, Supplier 6, Supplier 8, Supplier 12, Supplier 14, Supplier 32, and Supplier 40 have higher cost metrics ($61.39/unit, $64.15/unit, $52.47/unit, $69.82/unit, $56.9/unit, and $66.89/unit, respectively) and have these negotiation recommendations.\n",
      "\n",
      "7. **Supplier Type**: Retailers and distributors tend to have higher cost metrics compared to manufacturers. For example, Supplier 8, Supplier 28, Supplier 32, and Supplier 40 have higher cost metrics ($64.15/unit, $62.42/unit, $56.9/unit, and $66.89/unit, respectively) compared to Supplier 6 ($61.39/unit).\n",
      "\n",
      "8. **Payment Terms**: Suppliers with longer payment terms tend to have higher cost metrics. For example, Supplier 8, Supplier 14, and Supplier 32 have higher cost metrics ($64.15/unit, $69.82/unit, and $56.9/unit, respectively) and have longer payment terms (60 days, 60 days, and 60 days, respectively) compared to other suppliers.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n## Risk-Based Queries\")\n",
    "\n",
    "question = \"Which suppliers have the most non-compliance issues, regardless of risk level?\"\n",
    "answer = rag(question)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n## Compliance & Legal Queries:\")\n",
    "\n",
    "question = \"Identify suppliers where compliance monitoring is recommended.\"\n",
    "answer = rag(question)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n## Cost and Financial Metrics:\")\n",
    "\n",
    "question = \"List suppliers that offer the best cost metrics but are classified as high risk.\"\n",
    "answer = rag(question)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n## Contractual Terms and Recommendations:\")\n",
    "\n",
    "question = \"Which suppliers have penalty clauses for late delivery in their contracts, and what are the associated risks?\"\n",
    "answer = rag(question)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n## Supplier Relationship Queries:\")\n",
    "\n",
    "question = \"What are the relationship metrics for suppliers with the best past performance scores?\"\n",
    "answer = rag(question)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n## Opportunity and Innovation Queries:\")\n",
    "\n",
    "question = \"Identify suppliers with innovative solutions despite having poor quality metrics.\"\n",
    "answer = rag(question)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n# Custom Queries\")\n",
    "\n",
    "question = \"Show me all suppliers with a combination of poor quality metrics, high compliance issues, and good past performance scores.\"\n",
    "answer = rag(question)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "question = \"What are the patterns between compliance issues and cost metrics in supplier contracts?\"\n",
    "answer = rag(question)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87d9c7-7030-4aa2-85c4-475f38ff510c",
   "metadata": {},
   "source": [
    "**Retrieval Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbfa8de3-429b-470e-9ddb-ce3939eaec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9e0a5b5-ba46-425c-9bdd-f4743dd52fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question = pd.read_csv(r'/workspaces/Supply-Chain-Management/Data/ground-truth-retrieval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d80cba8-22c8-4707-987e-d3cd2d350788",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_question.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8370ef3f-0e1a-40a6-bac8-31a5d56cc9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5958c9aedb9542ea8660525dee4300ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.86384, 'mrr': 0.8502181587301586}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break  \n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def minsearch_search(query):\n",
    "    boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "evaluate(ground_truth, lambda q: minsearch_search(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91d819-281f-428f-8f67-1209b44d8ea4",
   "metadata": {},
   "source": [
    "**Best Retrieval Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b749d-08f0-4bb7-9781-06be625dd350",
   "metadata": {},
   "source": [
    "Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ded4ca2a-4f0f-441d-859f-42f0894aded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_validation, df_test = train_test_split(df_question, test_size=0.5, random_state=42)\n",
    "gt_val = df_validation.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4514d762-dbfa-411e-af2c-dcdb34b9d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search(query, boost=None):\n",
    "    if boost is None:\n",
    "        boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},  # Adjust filters if needed\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f31c9-45a1-4d33-91ce-a6983a765432",
   "metadata": {},
   "source": [
    "This function interacts with your search index. If no boost parameters are provided, it defaults to an empty dictionary. It returns the top 10 search results based on the query and the optional boost parameters. The boost_dict modifies the importance of specific fields during the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7a19140-e2eb-4f3f-9ebe-8c4575242a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_ranges = {\n",
    "    'supplier_name': (0.0, 3.0),\n",
    "    'supplier_type': (0.0, 3.0),\n",
    "    'risk_level': (0.0, 3.0),\n",
    "    'compliance_issues': (0.0, 3.0),\n",
    "    'key_terms': (0.0, 3.0),\n",
    "    'past_performance': (0.0, 3.0),\n",
    "    'negotiate_recommendation': (0.0, 3.0),\n",
    "    'supply_chain_disruption': (0.0, 3.0),\n",
    "    'quality_metrics': (0.0, 3.0),\n",
    "    'cost_metrics': (0.0, 3.0),\n",
    "}\n",
    "\n",
    "def objective(boost_params):\n",
    "    def search_function(q):\n",
    "        return minsearch_search(q['question'], boost_params)\n",
    "\n",
    "    results = evaluate(gt_val, search_function)\n",
    "    return results['mrr']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f162a7b-ca03-4e66-8eb3-54f9e12c8b2b",
   "metadata": {},
   "source": [
    "This dictionary defines the range of values that each parameter can take during optimization. The values indicate how much weight or importance is assigned to each of these fields when retrieving results from the search index.\n",
    "\n",
    "The function evaluates the effectiveness of a particular set of boost parameters. It does so by calling minsearch_search with the boost parameters and calculating the Mean Reciprocal Rank (MRR) over the validation set (gt_val). The higher the MRR score, the better the ranking of relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b4c8acc-86ef-4936-bf6e-2a50224c0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def simple_optimize(param_ranges, objective_function, n_iterations=10):\n",
    "    best_params = None\n",
    "    best_score = float('-inf')  # Assuming we're minimizing. Use float('-inf') if maximizing.\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Generate random parameters\n",
    "        current_params = {}\n",
    "        for param, (min_val, max_val) in param_ranges.items():\n",
    "            if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                current_params[param] = random.randint(min_val, max_val)\n",
    "            else:\n",
    "                current_params[param] = random.uniform(min_val, max_val)\n",
    "        \n",
    "        # Evaluate the objective function\n",
    "        current_score = objective_function(current_params)\n",
    "        \n",
    "        # Update best if current is better\n",
    "        if current_score > best_score:  # Change to > if maximizing\n",
    "            best_score = current_score\n",
    "            best_params = current_params\n",
    "    \n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f54d253-420a-4f52-a92c-f2e55cb0617a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b34fc0e0fb4b928f87b4fd4d66ded0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6512ce2007446abfa7cf9b6ccc9ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2cc58a972047488182e1019dc09018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bcc3232f6f429aaaa3c431d6af022c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0e89caae774746a920af19d5231749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f84e5b282647f0993bdc2ce76c2d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc045d648bac4f588c5ea56dba69fbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a600f83b674ab18284c4259dc3c75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73961fda70f490190433b826b63d605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d455c5c0c09c4c77a37190acee6e3879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4530d0fc081743ee819636be77f195b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542f4fda831f427297d8c4ea7e5d7123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6504adf64d4849a19447df5f44f577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ba1939e2be40b38a57cfec0bd112d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0901881ca0642279ac1231c633aeabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f21f88bc0b94632a463eb5ebe29f920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204011e8d74346db9ecee240d8b9622c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06bbaf3acd94b5f8bcdbb597b7b7941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88834b3cdb7a44e683bf33e57736066a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3cb357d6564b8ea9601169fff9af59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Boost Parameters: {'supplier_name': 2.8691396940069374, 'supplier_type': 1.8940336800638193, 'risk_level': 0.923442405280464, 'compliance_issues': 2.407565036605866, 'key_terms': 0.3208613898026407, 'past_performance': 1.4030636901968452, 'negotiate_recommendation': 0.7292841547630614, 'supply_chain_disruption': 1.650281040936573, 'quality_metrics': 0.15471113546029314, 'cost_metrics': 1.2247811572567455}\n",
      "Best MRR Score: 0.9346627936507936\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score = simple_optimize(param_ranges, objective, n_iterations=20)\n",
    "\n",
    "print(\"Best Boost Parameters:\", best_params)\n",
    "print(\"Best MRR Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858c06a-f10d-4610-a7b8-77c5ff8b0055",
   "metadata": {},
   "source": [
    "This is the optimized search function using the best boost parameters obtained from the optimization step. These parameters are applied to the search function, and it is evaluated using the full ground truth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e170b464-c6ef-4cbc-b9d0-8931253171aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c031ea13239490f9ece2a0db3175994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.93696, 'mrr': 0.9342981269841271}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minsearch_improved(query):\n",
    "    boost = {\n",
    "        'supplier_name': best_params['supplier_name'],\n",
    "        'supplier_type': best_params['supplier_type'],\n",
    "        'risk_level': best_params['risk_level'],\n",
    "        'compliance_issues': best_params['compliance_issues'],\n",
    "        'key_terms': best_params['key_terms'],\n",
    "        'past_performance': best_params['past_performance'],\n",
    "        'negotiate_recommendation': best_params['negotiate_recommendation'],\n",
    "        'supply_chain_disruption': best_params['supply_chain_disruption'],\n",
    "        'quality_metrics': best_params['quality_metrics'],\n",
    "        'cost_metrics': best_params['cost_metrics']\n",
    "    }\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results\n",
    "evaluate(ground_truth, lambda q: minsearch_improved(q['question']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc75592-fba1-468c-a768-81fc33645165",
   "metadata": {},
   "source": [
    "Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f003abce-d718-40d0-bc4e-a1c5e03fee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d4a4130-516f-41a7-a7ae-459b911531aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24101a802c75484b951ef76a4df55dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Hit Rate: 0.97296\n",
      "TF-IDF MRR: 0.9553497460317472\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Ensure the text is a string\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        # Additional preprocessing steps like removing punctuation can be added here\n",
    "        return text\n",
    "    return ''\n",
    "\n",
    "# Apply preprocessing\n",
    "df_question['processed_question'] = df_question['question'].apply(preprocess_text)\n",
    "\n",
    "# Prepare TF-IDF Vectorizer with custom settings\n",
    "corpus = df_question['processed_question'].tolist()\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),  # Bi-grams\n",
    "    stop_words='english',  # Use English stop words\n",
    "    sublinear_tf=True  # Sublinear term frequency scaling\n",
    ")\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "def tfidf_search(query, num_results=10):\n",
    "    query_processed = preprocess_text(query)\n",
    "    query_vec = vectorizer.transform([query_processed])\n",
    "    similarities = cosine_similarity(query_vec, X).flatten()\n",
    "    top_indices = np.argsort(similarities)[::-1][:num_results]\n",
    "    return df_question.iloc[top_indices].to_dict(orient='records')\n",
    "\n",
    "def evaluate_tfidf(ground_truth):\n",
    "    relevance_total = []\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        results = tfidf_search(q['question'])\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }\n",
    "\n",
    "# Evaluate TF-IDF retrieval approach\n",
    "tfidf_results = evaluate_tfidf(ground_truth)\n",
    "print('TF-IDF Hit Rate:', tfidf_results['hit_rate'])\n",
    "print('TF-IDF MRR:', tfidf_results['mrr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f8e07b5-7794-4176-a8b9-98e928cf7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best one among the the two \n",
    "def compare_methods(ground_truth, best_params):\n",
    "    # Evaluate TF-IDF\n",
    "    tfidf_results = evaluate_tfidf(ground_truth)\n",
    "    \n",
    "    # Evaluate Minsearch\n",
    "    minsearch_results = evaluate(ground_truth, best_params)\n",
    "\n",
    "    print(\"TF-IDF Results:\")\n",
    "    print('Hit Rate:', tfidf_results['hit_rate'])\n",
    "    print('MRR:', tfidf_results['mrr'])\n",
    "\n",
    "    print(\"Minsearch Results:\")\n",
    "    print('Hit Rate:', minsearch_results['hit_rate'])\n",
    "    print('MRR:', minsearch_results['mrr'])\n",
    "\n",
    "    if tfidf_results['mrr'] > minsearch_results['mrr']:\n",
    "        return 'TF-IDF', tfidf_results\n",
    "    else:\n",
    "        return 'Minsearch', minsearch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f3c6ed1a-a96e-4ac0-a996-2512b7af711e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bb9583d23d489bb4a914543e443210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45fba1bf5464a038b19ddb51068698e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Results:\n",
      "Hit Rate: 0.97296\n",
      "MRR: 0.9553497460317472\n",
      "Minsearch Results:\n",
      "Hit Rate: 0.93696\n",
      "MRR: 0.9342981269841271\n"
     ]
    }
   ],
   "source": [
    "results = compare_methods(ground_truth, lambda q: minsearch_improved(q['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b747c380-28c5-40e1-8dd7-e568dad15b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TF-IDF', {'hit_rate': 0.97296, 'mrr': 0.9553497460317472})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8c1745-3320-4ed8-907e-1c872b95caab",
   "metadata": {},
   "source": [
    "**RAG evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a179961a-83ad-4b9e-a1dd-314b1a9eddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f17517b-959d-48ce-a200-fd684f40ce0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ab18b4-67c5-4276-95df-6deb2d01e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b232bd-6cb7-4092-94a4-60048a19bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = record['question']\n",
    "answer_llm = rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72d688b4-1246-4e1b-8200-8015b276b68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supplier 1 has a high risk level and has had compliance issues with non-compliance with standards.\n"
     ]
    }
   ],
   "source": [
    "print(answer_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49608ac4-2542-4c28-849c-625a993dabc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert evaluator for a RAG system.\n",
      "Your task is to analyze the relevance of the generated answer to the given question.\n",
      "Based on the relevance of the generated answer, you will classify it\n",
      "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
      "\n",
      "Here is the data for evaluation:\n",
      "\n",
      "Question: What is the risk level of Supplier 1 and what compliance issues have they had?\n",
      "Generated Answer: Supplier 1 has a high risk level and has had compliance issues with non-compliance with standards.\n",
      "\n",
      "Please analyze the content and context of the generated answer in relation to the question\n",
      "and provide your evaluation in parsable JSON without using code blocks:\n",
      "\n",
      "{\n",
      "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
      "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt2_template.format(question=question, answer_llm=answer_llm)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39e98d91-4ab6-4f56-a818-475df561d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f153feb1-0068-40ba-8908-007a8c1a0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_question.sample(n=1250, random_state=1)\n",
    "sample = df_sample.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f57e5-2897-4f60-9598-70cfaebcba38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4e1d9e07714b10891db0dc65bea309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question) \n",
    "\n",
    "    prompt = prompt2_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt)\n",
    "    evaluation = json.loads(evaluation)\n",
    "\n",
    "    evaluations.append((record, answer_llm, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262948ad-86dc-4b37-9e54-3b5e34a113c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "del df_eval['record']\n",
    "del df_eval['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4531ea5-e9d6-4433-950b-8f9140b81a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.relevance.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a129b86-5441-47c3-9e6d-b9c6cf9b71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv(r'/workspaces/Supply-Chain-Management/Data/Evaluation/rag-eval-gpt-4o-mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b454e-96c2-4d07-930b-6ac386c4fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_eval[df_eval.relevance == 'NON_RELEVANT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8419cf-38e3-4f09-a102-c3cfef7eaae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluations_gpt4o = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question, model='gpt-4o') \n",
    "\n",
    "    prompt = prompt2_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt)\n",
    "    evaluation = json.loads(evaluation)\n",
    "    \n",
    "    evaluations_gpt4o.append((record, answer_llm, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520669d-359b-4062-aabf-a1b574f51f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_eval = pd.DataFrame(evaluations_gpt4o, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "del df_eval['record']\n",
    "del df_eval['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe34b5-ed2a-4700-9526-f85743beec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv(r'/workspaces/Supply-Chain-Management/Data/Evaluation/rag-eval-gpt-4o.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aaff2a-ceb1-4a17-b289-1ef281b4d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Pre-trained sentence similarity model for embedding-based similarity\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Pre-trained model for perplexity evaluation\n",
    "perplexity_model = pipeline(\"perplexity\", model=\"Llama3-groq-70b-8192-tool-use-preview\")\n",
    "\n",
    "# Weak supervision rule-based function (Modify the rules based on your data)\n",
    "def auto_classify_relevance(question, answer_llm):\n",
    "    \"\"\"\n",
    "    Automatically classify relevance based on simple rules or heuristics.\n",
    "    You can modify or expand this function.\n",
    "    \"\"\"\n",
    "    # Define rule-based heuristics for automatic classification\n",
    "    if \"important_keyword\" in answer_llm:  # Example rule based on keyword\n",
    "        return \"RELEVANT\"\n",
    "    elif len(answer_llm) > 50:  # Rule based on the length of the answer\n",
    "        return \"PARTLY_RELEVANT\"\n",
    "    else:\n",
    "        return \"NON_RELEVANT\"\n",
    "\n",
    "# Embedding-based similarity evaluation\n",
    "def compute_embedding_similarity(question, answer_llm):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between the question and the generated answer using sentence embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = embedding_model.encode([question, answer_llm])\n",
    "    similarity_score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return similarity_score\n",
    "\n",
    "# Perplexity-based evaluation\n",
    "def compute_perplexity(answer_llm):\n",
    "    \"\"\"\n",
    "    Compute perplexity score of the generated answer.\n",
    "    \"\"\"\n",
    "    perplexity_score = perplexity_model(answer_llm)\n",
    "    return perplexity_score\n",
    "\n",
    "# Evaluate on a sample set without manual relevance classification\n",
    "def evaluate_generated_answers(df_sample):\n",
    "    evaluations = []\n",
    "\n",
    "    for record in tqdm(df_sample.to_dict(orient='records')):\n",
    "        question = record['question']\n",
    "\n",
    "        # Generate answer with RAG system (Placeholder for your actual RAG system call)\n",
    "        answer_llm = rag(question)  # Replace with the RAG system call\n",
    "\n",
    "        # Automatic classification using weak supervision rules\n",
    "        relevance = auto_classify_relevance(question, answer_llm)\n",
    "\n",
    "        # Compute embedding similarity\n",
    "        similarity_score = compute_embedding_similarity(question, answer_llm)\n",
    "\n",
    "        # Compute perplexity\n",
    "        perplexity_score = compute_perplexity(answer_llm)\n",
    "\n",
    "        evaluations.append({\n",
    "            'id': record['id'],\n",
    "            'question': question,\n",
    "            'answer': answer_llm,\n",
    "            'relevance': relevance,\n",
    "            'similarity_score': similarity_score,\n",
    "            'perplexity': perplexity_score\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(evaluations)\n",
    "\n",
    "# Sample data from the ground truth (Assuming 12,500 records)\n",
    "df_sample = df_question.sample(n=1250, random_state=1)\n",
    "\n",
    "# Perform automatic RAG evaluation\n",
    "df_eval_auto = evaluate_generated_answers(df_sample)\n",
    "\n",
    "# Analyze the results\n",
    "print(df_eval_auto['relevance'].value_counts(normalize=True))\n",
    "print(df_eval_auto[['similarity_score', 'perplexity']].describe())\n",
    "\n",
    "# Save results to CSV\n",
    "df_eval_auto.to_csv(r'/workspaces/Supply-Chain-Management/Data/Evaluation/automatic_rag_evaluation.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
